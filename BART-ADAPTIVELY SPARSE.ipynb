{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMjOgdhOwoaqyROrou84M4o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"mHla9lYuGDjx","executionInfo":{"status":"ok","timestamp":1698223069175,"user_tz":-330,"elapsed":8,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D0YadGGqpf4g","executionInfo":{"status":"ok","timestamp":1698223069175,"user_tz":-330,"elapsed":16362,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}},"outputId":"a88a0467-d30e-4b5f-c55f-25677e2a3eb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"]}],"source":["!pip install transformers\n"]},{"cell_type":"code","source":["!pip install accelerate -U\n","!pip install transformers[torch]\n","!pip install datasets\n","!pip install dataset\n","!pip install entmax\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJi6rsSDeaJs","executionInfo":{"status":"ok","timestamp":1698223122978,"user_tz":-330,"elapsed":53810,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}},"outputId":"4fbee8b6-6929-4927-bc1e-97cb083aab43"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/261.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/261.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.24.0\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n","Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: dill, multiprocess, datasets\n","Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15\n","Collecting dataset\n","  Downloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n","Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n","  Downloading SQLAlchemy-1.4.49-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/alembic/\u001b[0m\u001b[33m\n","\u001b[0mCollecting alembic>=0.6.2 (from dataset)\n","  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting banal>=1.0.1 (from dataset)\n","  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n","Collecting Mako (from alembic>=0.6.2->dataset)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.0.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.1.3)\n","Installing collected packages: banal, sqlalchemy, Mako, alembic, dataset\n","  Attempting uninstall: sqlalchemy\n","    Found existing installation: SQLAlchemy 2.0.22\n","    Uninstalling SQLAlchemy-2.0.22:\n","      Successfully uninstalled SQLAlchemy-2.0.22\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.49 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Mako-1.2.4 alembic-1.12.0 banal-1.0.6 dataset-1.6.2 sqlalchemy-1.4.49\n","Collecting entmax\n","  Downloading entmax-1.1-py3-none-any.whl (12 kB)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from entmax) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->entmax) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->entmax) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->entmax) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->entmax) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->entmax) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->entmax) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->entmax) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->entmax) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->entmax) (1.3.0)\n","Installing collected packages: entmax\n","Successfully installed entmax-1.1\n"]}]},{"cell_type":"code","source":["import torch\n","from typing import List, Optional, Tuple, Union\n","from torch.autograd import grad\n","from torch import nn\n","from torch.nn.functional import softmax\n","from entmax import sparsemax, entmax15, entmax_bisect\n","from transformers.models.bart.modeling_bart import BartModel, BartAttention, BartForQuestionAnswering\n","\n","class ASTAttention(BartAttention):\n","  def __init__(self, config):\n","    super().__init__(\n","        embed_dim=config.d_model,\n","        num_heads=config.encoder_attention_heads,\n","        dropout=config.attention_dropout\n","      )\n","\n","  def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        key_value_states: Optional[torch.Tensor] = None,\n","        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        layer_head_mask: Optional[torch.Tensor] = None,\n","        output_attentions: bool = False,\n","    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n","        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n","\n","        # if key_value_states are provided this layer is used as a cross-attention layer\n","        # for the decoder\n","        is_cross_attention = key_value_states is not None\n","\n","        bsz, tgt_len, _ = hidden_states.size()\n","\n","        # get query proj\n","        query_states = self.q_proj(hidden_states) * self.scaling\n","        # get key, value proj\n","        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n","        # is checking that the `sequence_length` of the `past_key_value` is the same as\n","        # the provided `key_value_states` to support prefix tuning\n","        if (\n","            is_cross_attention\n","            and past_key_value is not None\n","            and past_key_value[0].shape[2] == key_value_states.shape[1]\n","        ):\n","            # reuse k,v, cross_attentions\n","            key_states = past_key_value[0]\n","            value_states = past_key_value[1]\n","        elif is_cross_attention:\n","            # cross_attentions\n","            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n","        elif past_key_value is not None:\n","            # reuse k, v, self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n","            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n","        else:\n","            # self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","\n","        if self.is_decoder:\n","            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n","            # Further calls to cross_attention layer can then reuse all cross-attention\n","            # key/value_states (first \"if\" case)\n","            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n","            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n","            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n","            # if encoder bi-directional self-attention `past_key_value` is always `None`\n","            past_key_value = (key_states, value_states)\n","\n","        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n","        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n","        key_states = key_states.reshape(*proj_shape)\n","        value_states = value_states.reshape(*proj_shape)\n","\n","        src_len = key_states.size(1)\n","        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n","\n","        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n","            raise ValueError(\n","                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n","                f\" {attn_weights.size()}\"\n","            )\n","\n","        if attention_mask is not None:\n","            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n","                raise ValueError(\n","                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n","                )\n","            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n","            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","\n","        attn_weights = softmax.entmax15(attn_weights, dim=-1)\n","\n","        if layer_head_mask is not None:\n","            if layer_head_mask.size() != (self.num_heads,):\n","                raise ValueError(\n","                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n","                    f\" {layer_head_mask.size()}\"\n","                )\n","            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n","            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n","\n","        if output_attentions:\n","            # this operation is a bit awkward, but it's required to\n","            # make sure that attn_weights keeps its gradient.\n","            # In order to do so, attn_weights have to be reshaped\n","            # twice and have to be reused in the following\n","            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n","            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n","        else:\n","            attn_weights_reshaped = None\n","\n","        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n","\n","        attn_output = torch.bmm(attn_probs, value_states)\n","\n","        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n","            raise ValueError(\n","                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n","                f\" {attn_output.size()}\"\n","            )\n","\n","        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n","        attn_output = attn_output.transpose(1, 2)\n","\n","        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n","        # partitioned across GPUs when using tensor-parallelism.\n","        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n","\n","        attn_output = self.out_proj(attn_output)\n","\n","        return attn_output, attn_weights_reshaped, past_key_value\n","\n","# model = BartModel.from_pretrained('facebook/bart-base')\n","model = BartForQuestionAnswering.from_pretrained('a-ware/bart-squadv2')\n","model.model.encoder.attention = ASTAttention(model.config)\n","model.model.decoder.attention = ASTAttention(model.config)"],"metadata":{"id":"bDfMMxm7qSjR","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["de435e3068c142ca80c99e7658ca1f5c","ff90a88a9e9443ae8479653c5bd6a1b7","d3df26efc51c49879445c35beb8852c4","d5414987eb5e48249a465a9b790e3e35","05533a43ab954386a295884a26e738ae","e69bcfd3f23746eca21a0db3328fa48b","2b309d7f933d48cea22e7ee59c78a281","9dc69a8f721f497e8fc51b6d29fe2aef","e79e914ceef747289c6f974b1b1ef313","d3f9935f902f48b7806123ed462eee4a","dc629c8d82434de5bbab5ea245fd5000","4cc31bf71cad48cc9ca234ac356debe4","246650cb02f14bb68892b6106962df04","0b27602af5a4456e90c7a0ae10b0b12c","9e666996a41f4c78807fdc1e5a45f58a","5cc36009b70149e3834ae7ab355e613e","f96daca2b71f447ba7a7c41eb259643f","911d2d04cc5b4054b3db9e7244c47216","c1a6f4b70bd646f99ae8970e247590de","4faecc278a4942aaa72a90646c875cbc","47e617ba2e2c4c9ab4b41f751bbad1df","f6e6c04dffe74eee81a11e9cd7e6dc10"]},"executionInfo":{"status":"ok","timestamp":1698223156969,"user_tz":-330,"elapsed":34011,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}},"outputId":"38fdf049-1a4e-4314-c009-f61b3200128c"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.16k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de435e3068c142ca80c99e7658ca1f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cc31bf71cad48cc9ca234ac356debe4"}},"metadata":{}}]},{"cell_type":"code","source":["from transformers import BartTokenizer, Trainer, TrainingArguments, DefaultDataCollator, AutoTokenizer, DefaultDataCollator\n","from datasets import load_dataset\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n","tokenizer = AutoTokenizer.from_pretrained('a-ware/bart-squadv2')\n","\n","def preprocess_function(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=384,\n","        truncation=\"only_second\",\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    for i, offset in enumerate(offset_mapping):\n","        answer = answers[i]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully inside the context, label it (0, 0)\n","        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs\n","\n","squad = load_dataset(\"squad\")\n","\n","## sample dataset to reduce training time.\n","squad[\"train\"] = squad[\"train\"].select([i for i in range(8000)])\n","squad[\"validation\"] = squad[\"validation\"].select([i for i in range(2000)])\n","\n","tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n","data_collator = DefaultDataCollator()\n","\n","training_args = TrainingArguments(\n","    output_dir=\"my_awesome_qa_model\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=6,\n","    per_device_eval_batch_size=6,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_squad[\"train\"],\n","    eval_dataset=tokenized_squad[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n"],"metadata":{"id":"drD2p907trGq","colab":{"base_uri":"https://localhost:8080/","height":684,"referenced_widgets":["aae75dcbff0b4d82bbc0560bf12be599","4f101e7ef94d4a1bb1911dae97ab35a4","efc3f485cb8a44fa92dc65355687e099","bd721e37675140baa6e5b5a3a79e1958","242c8828cb004fd587c17b1c1840c356","8972371667df458dbfe8e3cbd3e9fb9b","d957e056698a4e57a7a1db9057cc87f3","bfeeae04fc524818881f6527cff96533","d1e2f6e26e9a47668b4b2d93e9fcd034","f9233e3b314b42859ddc6c250c62cf7a","a85f7460246f4b15baf5f1fbf5d8f7c9","3daef487a5af456d95812d6d835dc215","a101aaae5dba45dd9efdf3feadf2a441","7574d72b65ec447f80b888c1ab7f5157","8e17aea2ee9c42309e08cfb252443951","4b960e99492747879c7a4e6aa3210c60","a1530e75c67a41c6b0b7589838311fb0","575ef299eb5f457da03f0ee65ccb79be","085de65295bd42a7b60c05b240f9f325","a9607e13a8134c51a4c12f0d059afb9e","bbb518b05b3b4b8994cfdf1479fd4c2b","e5e2eb37beef4c2aa45fc5a3f57d3c3b","9bebfd8c915c4f80a1374c910eb859bf","da0389fcde824190b4808848dfc8654e","daee4d8ae42b4491bb0e775b2d2db79f","ac95ded965ee441583ba3f6309ca0e3a","69ba497ad70f4918b72129621f885ec6","70d29555f91f49fca9479af309e7ef47","bc2964c9b2fc4b22b259ceb4c4a881b2","61757c562fa84797ade52828181f75e3","d150f5f944c54060a12ab792c6c95667","e4059fa7e9264a1ebc57f4f876311010","104fcbfe579d4a46ad6268d99d1fb785","d2fe5c77f47b44828890fbbdac48d41a","b351ee916fff48adbf06ede62c875836","ea7f182a51db497b912debf939bd3e02","71eae9295cd744c58a60e5377e75f720","ec760a17a24b4641b0ca877507386fec","53be7b7c0cc14de387de5487abd230b5","52b3c9de232c4a00b2cb7d55aa7a0ee7","63b36964765645abaaac94d530b9b0ba","9a93edb4efd04f5ea13e8cc4ccde355b","f23c40b142e44a5591dc405b977293ea","26443cc3d4504909935ffec413e58eeb","560abbf48ba84357accaca6a4e693fd5","5378dafdb3a14c2d99040d862f897f88","fe1390f976864bee95bbe9d206c072f3","22397762149f43a0ad9447cc7dbc9f2a","81d539e2582d48478c77972cd9523e7d","ef577674dbfd433e84d3c509e678eaa4","8d71bc6815f14627ba5255d8de8855d2","73873f772de1473fb32cba9373d5baff","e190ad0ff24a4ab7a22de04fd3e9366c","12250f9aa4f1436db431aeb03995aa8b","dc6985664e8547f2b3a3c15d33b67cbb","9080ecb8e3eb450b9156573ec8ce9d7f","2ffddd9580934a6a9495de125d470f2e","e8acf2c479174203ad02b669d3b3cf50","80f9b532f5854b449ce329e739e9b517","046f4b2a26f84bd6b1099683fb2b9165","bb478528fae940e1a742a8e463a99a8e","8b151f04cd8845a6aecd4bc0241ded0f","5f09cc3437d44d93a2b23b1a8e3c4aee","abf1328a2547438b9a4c4353ff321545","4fafc88575c84a84a90af492b88d5ad2","73de1431a5aa48f4b0ae59839679a472","0d56131122094f5f968e1a059ab81ab0","bd137d4c33e443fda4c7225a37218649","31bd0c34c8624d6dbb83036710b9759f","d5b0a444f33f491a87bcd4cb8934c989","fb41784aaeea435d83287907f3806488","d3cb7a126e944ac5918fb4e7077f3649","c4f6d983ac4f456ba7f12f60176db53d","a73ee2a7867743d5963119a7f78bb660","7c7344ea2a024800a765bee98763214b","56be68b68d544b6fbf6ef6880832ae3b","d781faf1c3d4469b809dd197fa158929","39ba38ee8b704bcfafb1d79e84bb0101","43992d334f22484c974171e483f11026","60971746befc4b508aa4c6db48621f98","4165003a20b542689438b29d257acaba","a981c724135c4bc4acbda0a543f80f76","7ec7fcf031ef4fb88784cc2a6a29de8c","9a50a04a973149fa811a50adc8cef987","964486231a0546e996c72933a5664fa8","47b4b43fd14f4a4fbbd887659cf6692d","7543e0eef12845efbc882fa914543bbb","f33d05ba019542479eeb59e6f38319ab","60dc7e6d55c14baf97a918a045f0d58a","86186eb449ae4a3a96c484400bc0c0a2","e8687105def749beb9ff7af1f08f037d","b440134b8a7849ceb10e21def1cf0bcd","800b64d24daa4a2f869580b21167e262","6be039d08b83403b8c4024053689b69e","fddae610f7374615b2fc822028b12653","119f232ab9dc4bd09f929ef73fdf4f76","6bb2b279b03943d08367439679234e84","c93c08495e5e4b6aa1d7529e3b5fb19e","ee87261345594b799ce27c6a0e56712c","a183406702b14f21bc1a05c9ec189084","5ca6b4d16fe24702adf214a26dc82fc4","86258e3777b44b3599b0f5ae7aff74ae","0e90968f628e48cb8cf0efd82fa2369a","35867ab913fa412e88419ec5a5d05b4a","fe7f310e44ce4662a301e6638a032ba7","79087f0e73644b939195a7f31adbf098","b318f14d785741ed898298d6d3ada850","ded6848ea42e43b9b85272a881d13d8e","330c3e85f38c4609b9214574d7251da0","c6cd6caec8f544089a88986df6ae7ef0","72a2d41505d4472590182655b2e59293","f951e0e47c394fcf9adcebf7a7df2b6a","83b29833ab4e48f385e3d1a3b542d398","9d5894fc70b54bc3b1b40f433baa3577","8d689e9ca67f496fa90a8355cd63530e","a38197fb861d464f9173f1457bdc595b","a90743b1212b4f569fb8fcbb35823e02","474e29d7b61a468eaa2c5538c59318c9","88ceab0361464fa2a7276c5d3607e67e","fd039b052073493cabeac125512efe9d","caaab9fa5adb4754b5f0e12059576129","2a8b64c1618f4d8392c1ecb51a2c45c1","60284faedb51469a959fbe7336183acd","545bcfddad974655a9a4ef26d29de829","3e3cd0b4930440c48db2f501cb0bd560","44a9148d4a0248adbb857c127f2327ba","18b9305539944c94bfb3afa3d1937a8b","697bde65fbe2416aba15b52ecec074bd","39fde687d67c4bcc853556830764ca25","2e67df8a5c1f46e4bf9b2c31584c011e","504740a12c574e7d964411b38fb7a46e","d5e5fe60bb50410f91cdc78a76d3d4a1","3d111cecc27a4857940416d2d6673597","5725d0116cce4974afd15bd91f5f9581","4d5743be642e47c0a93eb7d40bf75f01","d90ea2eb58c245838b58b11bd1ef08e9","ffac7cf17a2243a4b907030a84ec652c","02582891795444deafae78a4ffacdd69","1c0be7398beb4fbe9865d3611551456d","0694b86e2d924b999a302710c6a4cdf6","bf7c7e2da7c74d88839f8756c7f04a52","bd3bf30a6b1d4f24acbfce23dda6b68f","4f591f5d0a4545da92db5234aff8bb58","1ef95ddcd4444249a3f79527fd7226c1","46efb6d1537d44099d9988d8f13a6c2f","671d219cb4a749a9aae16d29647e8f9a","8ea035db027d497b930bcad0e50fa0f7","9d018a6af0714d20ad70b0b8723fb7ba","26a9672ff75a40399568adabbf775c2f","4d0bfc225371453e92baa7a253cb8c4a","86d7e16a5dd74afb9e615f9d9b23168a","979fa151bda642eb9917f11965c5aea5","91edc3349bb54e31928af6813dea88fd","c1d1c6131d5049d098b4e47c3a66a5b1","b3417ea473714db48b2d00b6fc9a7add","ad0a97b1d2994a9d958344a11dea2d7a","1deea93ee1ba49c080c8e9abd86aaa29","ff6f904abd9b4685a3877ceec4b7d5d0","63c66924ca144b15bdcbf16e8ffcc84e","5fd1ff1e947843529887b59eac8901f0","0e639bc5dc9e4560893d8d362f76af60","0d30d43dedcb4773be592f657b2e3424","9db52c04f3b9415eb47614ae894564ba","7624c1de28814d378756049ef48a3637","b19f18205c954c2cb6b7f1df1eb8375d"]},"outputId":"9e36ead5-65b7-4389-fa88-28f755e8bbc7","executionInfo":{"status":"ok","timestamp":1698231400605,"user_tz":-330,"elapsed":8243649,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}}},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae75dcbff0b4d82bbc0560bf12be599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3daef487a5af456d95812d6d835dc215"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bebfd8c915c4f80a1374c910eb859bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2fe5c77f47b44828890fbbdac48d41a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"560abbf48ba84357accaca6a4e693fd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading metadata:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9080ecb8e3eb450b9156573ec8ce9d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/7.67k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d56131122094f5f968e1a059ab81ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39ba38ee8b704bcfafb1d79e84bb0101"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60dc7e6d55c14baf97a918a045f0d58a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a183406702b14f21bc1a05c9ec189084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a2d41505d4472590182655b2e59293"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a8b64c1618f4d8392c1ecb51a2c45c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d111cecc27a4857940416d2d6673597"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef95ddcd4444249a3f79527fd7226c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3417ea473714db48b2d00b6fc9a7add"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4002' max='4002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4002/4002 2:16:23, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.468900</td>\n","      <td>1.087335</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.282300</td>\n","      <td>1.400288</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.130000</td>\n","      <td>1.681610</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=4002, training_loss=0.29918319792270304, metrics={'train_runtime': 8187.6612, 'train_samples_per_second': 2.931, 'train_steps_per_second': 0.489, 'total_flos': 1.9968364228608e+16, 'train_loss': 0.29918319792270304, 'epoch': 3.0})"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["model.save_pretrained(save_directory=\"./qa_model_mini\")"],"metadata":{"id":"szVEHzziawpn","executionInfo":{"status":"ok","timestamp":1698231417739,"user_tz":-330,"elapsed":17139,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# testing\n","import torch\n","from transformers.models.bart.modeling_bart import BartModel, BartForQuestionAnswering\n","from transformers import AutoTokenizer\n","\n","# testing\n","tokenizer = AutoTokenizer.from_pretrained(\"a-ware/bart-squadv2\")\n","question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n","encoding = tokenizer(question, text, return_tensors='pt')\n","model = BartForQuestionAnswering.from_pretrained(pretrained_model_name_or_path = \"./qa_model_mini/\")\n","with torch.no_grad():\n","    outputs = model(**encoding)\n","answer_start_index = outputs.start_logits.argmax()\n","answer_end_index = outputs.end_logits.argmax()\n","predict_answer_tokens = encoding.input_ids[0, answer_start_index : answer_end_index + 1]\n","\n","model_answer_decoded = tokenizer.decode(predict_answer_tokens)\n","print(\"question => \" + question)\n","print(\"model's answer => \" + model_answer_decoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lex-10BuKJt","executionInfo":{"status":"ok","timestamp":1698233220595,"user_tz":-330,"elapsed":15481,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}},"outputId":"ae7a2725-8088-4b62-fcde-0fe7f10d5fa2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n","Some weights of the model checkpoint at ./qa_model_mini/ were not used when initializing BartForQuestionAnswering: ['model.decoder.attention.q_proj.weight', 'model.encoder.attention.q_proj.weight', 'model.encoder.attention.out_proj.weight', 'model.encoder.attention.v_proj.weight', 'model.decoder.attention.k_proj.bias', 'model.encoder.attention.v_proj.bias', 'model.decoder.attention.out_proj.bias', 'model.encoder.attention.out_proj.bias', 'model.decoder.attention.k_proj.weight', 'model.encoder.attention.k_proj.weight', 'model.decoder.attention.out_proj.weight', 'model.encoder.attention.k_proj.bias', 'model.encoder.attention.q_proj.bias', 'model.decoder.attention.v_proj.weight', 'model.decoder.attention.v_proj.bias', 'model.decoder.attention.q_proj.bias']\n","- This IS expected if you are initializing BartForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BartForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["question => Who was Jim Henson?\n","model's answer =>  puppet\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!mkdir /content/drive/MyDrive/squadv2_dataset\n","!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/v2.0/squad2.0_test.json -O /content/drive/MyDrive/squadv2_dataset/squadv2_test_set.json\n","!unzip /content/drive/MyDrive/squadv2_dataset/squadv2_test_set.json.zip -d /content/drive/MyDrive/squadv2_dataset/\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FZzS418WuhSd","executionInfo":{"status":"ok","timestamp":1698235700929,"user_tz":-330,"elapsed":20215,"user":{"displayName":"Satyam Sharan","userId":"09078711455331337386"}},"outputId":"feabc4cc-c010-4b07-d539-046d84cd8534"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","--2023-10-25 12:10:06--  https://rajpurkar.github.io/SQuAD-explorer/dataset/v2.0/squad2.0_test.json\n","Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 404 Not Found\n","2023-10-25 12:10:06 ERROR 404: Not Found.\n","\n","unzip:  cannot find or open /content/drive/MyDrive/squadv2_dataset/squadv2_test_set.json.zip, /content/drive/MyDrive/squadv2_dataset/squadv2_test_set.json.zip.zip or /content/drive/MyDrive/squadv2_dataset/squadv2_test_set.json.zip.ZIP.\n"]}]}]}
